{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "\n",
    "# The notebook below contains code to batch generate glacier flowlines from GrIMP velocity data in a loop, and prepare sampling points for velocity extraction.\n",
    "\n",
    "# Note that that we experienced kernel crashes when trying to process too many glaciers at once (in excess of 15). We recommand splitting the input box coordinates CSVs into smaller chunks and run this notebook multiple times with different input files.\n",
    "\n",
    "# The notebook involves:\n",
    "# 1. Importing necessary libraries and setting up directories\n",
    "# 2. Defining functions for generating flowlines\n",
    "# 3. Main processing loop: \n",
    "  # - parameter setup,\n",
    "  # - generating seed points from input box coordinates,\n",
    "  # - generating flowlines for each glacier,\n",
    "  # - saving flowlines as points and lines shapefiles, and CSVs,\n",
    "  # - visualising flowlines over velocity data as points and lines,\n",
    "  # - collecting and saving flowline statistics for each glacier individually and for all glaciers processed in a given chunk combined.\n",
    "# 4. Extraction of flowlines manually picked for further analysis (manual picking perfmormed in QGIS)\n",
    "# 5. Visualisation of manually picked flowlines over velocity data as lines and buffered areas.\n",
    "# 6. Cataloguing of sampling points for velocity extraction by glacier, flowline, and elevation buffer (sampling points chosen at random within an elevation buffer in QGIS\n",
    "\n",
    "# This file uses glacier-flow-tools library from the PISM project (https://github.com/pism/glacier-flow-tools.git), licensed under GNU GPLv3.\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gp\n",
    "from shapely import Point\n",
    "import pylab as plt\n",
    "\n",
    "from glacier_flow_tools.interpolation import velocity\n",
    "from glacier_flow_tools.pathlines import (\n",
    "    compute_pathline,\n",
    "    series_to_pathline_geopandas_dataframe,\n",
    "    pathline_to_line_geopandas_dataframe,\n",
    ")\n",
    "from glacier_flow_tools.utils import register_colormaps\n",
    "register_colormaps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute_pathline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Tuple, Union\n",
    "\n",
    "import geopandas as gp\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from numpy.linalg import norm\n",
    "from shapely.geometry import LineString, Point\n",
    "from tqdm import tqdm as tqdm_script\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from xarray import DataArray\n",
    "\n",
    "from glacier_flow_tools.gaussian_random_fields import (\n",
    "    distrib_normal,\n",
    "    generate_field,\n",
    "    power_spectrum,\n",
    ")\n",
    "from glacier_flow_tools.geom import distances\n",
    "\n",
    "def compute_pathline(\n",
    "    point: Union[list, ndarray, Point],\n",
    "    f: Callable,\n",
    "    f_args: Tuple,\n",
    "    start_time: float = 0.0,\n",
    "    end_time: float = 1000.0,\n",
    "    hmin: float = 0.0001,\n",
    "    hmax: float = 10,\n",
    "    tol: float = 1e-4,\n",
    "    v_threshold: float = 0.0,\n",
    "    notebook: bool = False,\n",
    "    progress: bool = False,\n",
    "    progress_kwargs: Dict = {\"leave\": False, \"position\": 0},\n",
    ") -> Tuple[ndarray, ndarray, ndarray, ndarray]:\n",
    "    \"\"\"\n",
    "    Compute a pathline using Runge-Kutta-Fehlberg integration.\n",
    "\n",
    "    This function computes a pathline, which is a trajectory traced by a particle in a fluid flow. The pathline is computed by integrating the velocity field using the Runge-Kutta-Fehlberg method. The function is unit-agnostic, requiring the user to ensure consistency of units. For example, if the velocity field is given in m/yr, the `start_time` and `end_time` are assumed to be in years.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    point : list, ndarray, or shapely.Point\n",
    "        Starting point of the pathline.\n",
    "    f : callable\n",
    "        Function that computes the derivative of the state at a given point.\n",
    "    f_args : tuple\n",
    "        Additional arguments to pass to the function `f`.\n",
    "    start_time : float, optional\n",
    "        The start time of integration. Default is 0.0.\n",
    "    end_time : float, optional\n",
    "        The end time of integration. Default is 1000.0.\n",
    "    hmin : float, optional\n",
    "        The minimum step size for the integration. Default is 0.0001.\n",
    "    hmax : float, optional\n",
    "        The maximum step size for the integration. Default is 10.\n",
    "    tol : float, optional\n",
    "        The error tolerance for the integration. Default is 1e-4.\n",
    "    v_threshold : float, optional\n",
    "        A velocity threshold below which the solver stops. Default is 0.\n",
    "    notebook : bool, optional\n",
    "        If True, a progress bar is displayed in a Jupyter notebook. Default is False.\n",
    "    progress : bool, optional\n",
    "        If True, a progress bar is displayed. Default is False.\n",
    "    progress_kwargs : dict, optional\n",
    "        Additional keyword arguments for the progress bar. Default is {\"leave\": False, \"position\": 0}.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pts : ndarray\n",
    "        The points along the pathline.\n",
    "    velocities : ndarray\n",
    "        The velocity at each point along the pathline.\n",
    "    pts_error_estimate : ndarray\n",
    "        Error estimate at each point along the pathline.\n",
    "\n",
    "    Examples\n",
    "    ----------\n",
    "    >>> import numpy as np\n",
    "    >>> from shapely.geometry import Point\n",
    "    >>> def velocity_field(point, t):\n",
    "    >>>     x, y = point\n",
    "    >>>     return np.array([-y, x])\n",
    "    >>> point = [1, 0]\n",
    "    >>> pts, v, _ = compute_pathline(point, velocity_field, (), start_time=0, end_time=2*np.pi)\n",
    "    \"\"\"\n",
    "\n",
    "    a2 = 2.500000000000000e-01  #  1/4\n",
    "    a3 = 3.750000000000000e-01  #  3/8\n",
    "    a4 = 9.230769230769231e-01  #  12/13\n",
    "    a5 = 1.000000000000000e00  #  1\n",
    "    a6 = 5.000000000000000e-01  #  1/2\n",
    "\n",
    "    b21 = 2.500000000000000e-01  #  1/4\n",
    "    b31 = 9.375000000000000e-02  #  3/32\n",
    "    b32 = 2.812500000000000e-01  #  9/32\n",
    "    b41 = 8.793809740555303e-01  #  1932/2197\n",
    "    b42 = -3.277196176604461e00  # -7200/2197\n",
    "    b43 = 3.320892125625853e00  #  7296/2197\n",
    "    b51 = 2.032407407407407e00  #  439/216\n",
    "    b52 = -8.000000000000000e00  # -8\n",
    "    b53 = 7.173489278752436e00  #  3680/513\n",
    "    b54 = -2.058966861598441e-01  # -845/4104\n",
    "    b61 = -2.962962962962963e-01  # -8/27\n",
    "    b62 = 2.000000000000000e00  #  2\n",
    "    b63 = -1.381676413255361e00  # -3544/2565\n",
    "    b64 = 4.529727095516569e-01  #  1859/4104\n",
    "    b65 = -2.750000000000000e-01  # -11/40\n",
    "\n",
    "    r1 = 2.777777777777778e-03  #  1/360\n",
    "    r3 = -2.994152046783626e-02  # -128/4275\n",
    "    r4 = -2.919989367357789e-02  # -2197/75240\n",
    "    r5 = 2.000000000000000e-02  #  1/50\n",
    "    r6 = 3.636363636363636e-02  #  2/55\n",
    "\n",
    "    c1 = 1.157407407407407e-01  #  25/216\n",
    "    c3 = 5.489278752436647e-01  #  1408/2565\n",
    "    c4 = 5.353313840155945e-01  #  2197/4104\n",
    "    c5 = -2.000000000000000e-01  # -1/5\n",
    "\n",
    "    if isinstance(point, Point):\n",
    "        point = np.squeeze(np.array(point.coords.xy).reshape(1, -1))\n",
    "\n",
    "    x = point\n",
    "    t = start_time\n",
    "    h = hmax\n",
    "\n",
    "    pts = np.empty((0, len(x)), dtype=float)\n",
    "    velocities = np.empty((0, len(x)), dtype=float)\n",
    "    time = np.empty(0, dtype=float)\n",
    "    error_estimate = np.empty(0, dtype=float)\n",
    "\n",
    "    pts = np.vstack([pts, x])\n",
    "    vel = f(point, start_time, *f_args)\n",
    "    v = np.sqrt(vel[0] ** 2 + vel[1] ** 2)\n",
    "    velocities = np.vstack([velocities, vel])\n",
    "    time = np.append(time, start_time)\n",
    "    error_estimate = np.append(error_estimate, 0.0)\n",
    "\n",
    "    p_bar = tqdm_notebook if notebook else tqdm_script\n",
    "    with p_bar(desc=\"Integrating pathline\", total=end_time, **progress_kwargs) if progress else nullcontext():\n",
    "        while (t < end_time) and (v > v_threshold):\n",
    "\n",
    "            if np.isclose(t + h, end_time, rtol=1e-5):\n",
    "                h = end_time - t\n",
    "\n",
    "            k1 = h * f(x, t, *f_args)\n",
    "            if np.any(np.isnan(k1)):\n",
    "                return np.array([[np.nan, np.nan]]), np.array([[np.nan, np.nan]]), np.array([t]), np.array([np.nan])\n",
    "            k2 = h * f(x + b21 * k1, t + a2 * h, *f_args)\n",
    "            if np.any(np.isnan(k2)):\n",
    "                return np.array([[np.nan, np.nan]]), np.array([[np.nan, np.nan]]), np.array([t]), np.array([np.nan])\n",
    "            k3 = h * f(x + b31 * k1 + b32 * k2, t + a3 * h, *f_args)\n",
    "            if np.any(np.isnan(k3)):\n",
    "                return np.array([[np.nan, np.nan]]), np.array([[np.nan, np.nan]]), np.array([t]), np.array([np.nan])\n",
    "            k4 = h * f(x + b41 * k1 + b42 * k2 + b43 * k3, t + a4 * h, *f_args)\n",
    "            if np.any(np.isnan(k4)):\n",
    "                return np.array([[np.nan, np.nan]]), np.array([[np.nan, np.nan]]), np.array([t]), np.array([np.nan])\n",
    "            k5 = h * f(x + b51 * k1 + b52 * k2 + b53 * k3 + b54 * k4, t + a5 * h, *f_args)\n",
    "            if np.any(np.isnan(k5)):\n",
    "                return np.array([[np.nan, np.nan]]), np.array([[np.nan, np.nan]]), np.array([t]), np.array([np.nan])\n",
    "            k6 = h * f(x + b61 * k1 + b62 * k2 + b63 * k3 + b64 * k4 + b65 * k5, t + a6 * h, *f_args)\n",
    "            if np.any(np.isnan(k6)):\n",
    "                return np.array([[np.nan, np.nan]]), np.array([[np.nan, np.nan]]), np.array([t]), np.array([np.nan])\n",
    "\n",
    "            r = norm(r1 * k1 + r3 * k3 + r4 * k4 + r5 * k5 + r6 * k6) / h\n",
    "            if r <= tol:\n",
    "\n",
    "                pts = np.append(pts, [x], axis=0)\n",
    "                velocities = np.append(velocities, [vel], axis=0)\n",
    "                time = np.append(time, t)\n",
    "                error_estimate = np.append(error_estimate, r)\n",
    "\n",
    "                t = t + h\n",
    "                x = x + c1 * k1 + c3 * k3 + c4 * k4 + c5 * k5\n",
    "\n",
    "                vel = f(x, t, *f_args)\n",
    "                v = np.sqrt(vel[0] ** 2 + vel[1] ** 2)\n",
    "\n",
    "            s = (tol / r) ** 0.25\n",
    "            h = np.minimum(h * s, hmax)\n",
    "\n",
    "            if (h < hmin) and (t < end_time):\n",
    "                print(\n",
    "                    f\"Error: Could not converge to the required tolerance {tol:e} with minimum stepsize  {hmin:e} at t={t}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "    return pts, velocities, time, error_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Prossesing of Flowline Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BATCH PROCESSING FOR MULTIPLE GLACIERS ===\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "import geopandas as gp\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "from contextlib import nullcontext\n",
    "\n",
    "#Parameters for pathline computation\n",
    "hmin = 0.01\n",
    "hmax = 1\n",
    "tol = 0.1 #was 1 before\n",
    "start_time = 0\n",
    "end_time = 1000\n",
    "v_threshold = 100\n",
    "spacing = 200  # spacing for seed points, was 500 before, matches GrIMP mosaic resolution\n",
    "\n",
    "\n",
    "# Input CSV\n",
    "coords_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Input/Box Coordinates/box_sp_1_v3.csv'\n",
    "df_coords = pd.read_csv(coords_path)\n",
    "\n",
    "# For collecting stats later\n",
    "all_glacier_stats = []\n",
    "\n",
    "# === PROCESS EACH GLACIER ===\n",
    "for _, row in tqdm(df_coords.iterrows(), total=len(df_coords), desc=\"Processing glaciers\"):\n",
    "    try:\n",
    "        feature_id = row.get('feature_ID', 'unknown')\n",
    "        glacier_id = row.get('glacier_ID', 'unknown')\n",
    "        glacier_name = row.get('glacier_name', 'unknown')\n",
    "\n",
    "        sp_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Input/Starting Points'\n",
    "        os.makedirs(sp_path, exist_ok=True)\n",
    "\n",
    "        seed_points = []\n",
    "        seed_metadata = []\n",
    "        used_box_nums = set()\n",
    "\n",
    "        # Identify all box numbers dynamically (even if some unused)\n",
    "        box_nums = set()\n",
    "        for col in row.index:\n",
    "            if col.startswith(\"coord_1_\"):\n",
    "                num = col.split(\"_\")[2]\n",
    "                box_nums.add(num)\n",
    "\n",
    "        # For each box, generate seed points\n",
    "        for num in sorted(box_nums, key=int):\n",
    "            coord1_col = f\"coord_1_{num}\"\n",
    "            coord2_col = f\"coord_2_{num}\"\n",
    "\n",
    "            # Skip if any of the coords are NaN\n",
    "            if pd.isna(row.get(coord1_col)) or pd.isna(row.get(coord2_col)):\n",
    "                continue\n",
    "\n",
    "            # Parse coordinates from strings\n",
    "            x1_str, y1_str = str(row[coord1_col]).split(\",\")\n",
    "            x2_str, y2_str = str(row[coord2_col]).split(\",\")\n",
    "            x1, y1 = float(x1_str), float(y1_str)\n",
    "            x2, y2 = float(x2_str), float(y2_str)\n",
    "\n",
    "            # Order coordinates\n",
    "            x_start, x_end = sorted([x1, x2])\n",
    "            y_start, y_end = sorted([y1, y2])\n",
    "\n",
    "            # Generate ranges\n",
    "            x_coords = range(int(x_start), int(x_end), spacing)\n",
    "            y_coords = range(int(y_start), int(y_end), spacing)\n",
    "\n",
    "            # Create points and metadata\n",
    "            if x_coords and y_coords:\n",
    "                used_box_nums.add(num)\n",
    "\n",
    "            for x in x_coords:\n",
    "                for y in y_coords:\n",
    "                    seed_points.append(Point(x, y))\n",
    "                    seed_metadata.append({\n",
    "                        'glacier_ID': glacier_id,\n",
    "                        'feature_ID': feature_id,\n",
    "                        'glacier_name': glacier_name,\n",
    "                        'box_number': num,\n",
    "                        'x': x,\n",
    "                        'y': y\n",
    "                    })\n",
    "\n",
    "        print(f\"For Glacier {glacier_name} (ID:{feature_id}) generated {len(seed_points)} seeding points across {len(used_box_nums)} boxes.\")\n",
    "\n",
    "        # Build GeoDataFrame\n",
    "        glacier_gdf = gp.GeoDataFrame(seed_metadata, geometry=seed_points, crs=\"EPSG:3413\")\n",
    "\n",
    "        # Save GeoJSON\n",
    "        filename = f\"sp_{feature_id}.geojson\"\n",
    "        filepath = os.path.join(sp_path, filename)\n",
    "        glacier_gdf.to_file(filepath, driver='GeoJSON')\n",
    "\n",
    "        # Keep for next steps\n",
    "        starting_points_df = glacier_gdf\n",
    "\n",
    "        # Build GeoDataFrame\n",
    "        glacier_gdf = gp.GeoDataFrame(seed_metadata, geometry=seed_points, crs=\"EPSG:3413\")\n",
    "\n",
    "        # Save GeoJSON\n",
    "        filename = f\"sp_{feature_id}.geojson\"\n",
    "        filepath = os.path.join(sp_path, filename)\n",
    "        glacier_gdf.to_file(filepath, driver='GeoJSON')\n",
    "\n",
    "        # Keep for next steps\n",
    "        starting_points_df = glacier_gdf\n",
    "        \n",
    "\n",
    "        # === GENERATE FLOWLINES FROM SEEDING POINTS ===\n",
    "\n",
    "        # Load vx\n",
    "        with rasterio.open(\"data/kuba-flowlines/GL_vel_mosaic_Annual_01Dec15_30Nov21_vx_v05.0_nwcw_average.tif\") as src_vx:\n",
    "            Vx = src_vx.read(1)\n",
    "            transform = src_vx.transform\n",
    "            xres = transform.a\n",
    "            yres = -transform.e  # usually negative\n",
    "            width = src_vx.width\n",
    "            height = src_vx.height\n",
    "\n",
    "        # Load vy (assumed same grid)\n",
    "        with rasterio.open(\"data/kuba-flowlines/GL_vel_mosaic_Annual_01Dec15_30Nov21_vy_v05.0_nwcw_average.tif\") as src_vy:\n",
    "            Vy = src_vy.read(1)\n",
    "\n",
    "        # Load vv (assumed same grid)\n",
    "        with rasterio.open(\"data/kuba-flowlines/GL_vel_mosaic_Annual_01Dec15_30Nov21_vv_v05.0_nwcw_average.tif\") as src_vv:\n",
    "            Vv = src_vv.read(1)\n",
    "\n",
    "        # Reverse velocity for backward pathlines\n",
    "        Vx = -Vx\n",
    "        Vy = -Vy\n",
    "\n",
    "        # Build coordinate arrays\n",
    "        x = np.arange(width) * xres + transform.c\n",
    "        y = np.arange(height) * -yres + transform.f  # careful: flip if needed\n",
    "\n",
    "        pathlines = []\n",
    "        metadata = []\n",
    "\n",
    "        for idx, df in starting_points_df.iterrows():\n",
    "            pt = df.geometry.coords[0]\n",
    "            print(f\" → Processing seed point {idx+1}/{len(starting_points_df)}: {pt}\")\n",
    "            glacier_id = df.get(\"glacier_ID\", \"unknown\")\n",
    "            feature_id = df.get(\"feature_ID\", \"unknown\")\n",
    "\n",
    "            try:\n",
    "                pathline = compute_pathline(\n",
    "                    [*pt],\n",
    "                    velocity,\n",
    "                    f_args=(Vx, Vy, x, y),\n",
    "                    hmin=hmin,\n",
    "                    hmax=hmax,\n",
    "                    tol=tol,\n",
    "                    start_time=start_time,\n",
    "                    end_time=end_time,\n",
    "                    v_threshold=v_threshold,\n",
    "                    notebook=True,\n",
    "                    progress=False,\n",
    "                )\n",
    "                pathlines.append(pathline)\n",
    "                metadata.append(df.drop(\"geometry\", errors=\"ignore\"))\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Pathline failed for point {pt} feature_ID={feature_id}\")\n",
    "                print(f\"   → Reason: {e}\")\n",
    "\n",
    "\n",
    "        # === SAVE FLOWLINES AS POINTS ===\n",
    "\n",
    "        # Parameters\n",
    "        fl_points_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Flowlines/as Points'\n",
    "        os.makedirs(fl_points_path, exist_ok=True)\n",
    "\n",
    "        # Generate result GeoDataFrame\n",
    "        result = pd.concat(\n",
    "            [\n",
    "                series_to_pathline_geopandas_dataframe(\n",
    "                    s.drop(\"geometry\", errors=\"ignore\"), pathlines[i]\n",
    "                )\n",
    "                for i, (k, s) in enumerate(starting_points_df.iterrows())\n",
    "                if i < len(pathlines)\n",
    "            ]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # Filter out broken geometries\n",
    "        result = result[\n",
    "            (result.geometry.x > -700_000) & (result.geometry.x < 700_000) &\n",
    "            (result.geometry.y > -3_000_000) & (result.geometry.y < -1_000_000)\n",
    "        ]\n",
    "\n",
    "        # Save GeoJSON\n",
    "        filename = f\"fl_{feature_id}_points.geojson\"\n",
    "        filepath = os.path.join(fl_points_path, filename)\n",
    "        result.to_file(filepath, driver='GeoJSON')\n",
    "\n",
    "\n",
    "        # === SAVE FLOWLINES AS CSV ===\n",
    "\n",
    "        # Parameters\n",
    "        fl_csv_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Flowlines/as CSV'\n",
    "        os.makedirs(fl_csv_path, exist_ok=True)\n",
    "\n",
    "        import warnings\n",
    "\n",
    "        result_wkt = result.copy()\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            result_wkt[\"geometry\"] = result_wkt[\"geometry\"].apply(lambda geom: geom.wkt)\n",
    "\n",
    "        filename = f\"fl_{feature_id}_points_v3.csv\"\n",
    "        filepath = os.path.join(fl_csv_path, filename)\n",
    "        result_wkt.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "        # === SAVE FLOWLINES AS LINES ===\n",
    "\n",
    "        # Parameters\n",
    "        fl_lines_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Flowlines/as Lines'\n",
    "        os.makedirs(fl_lines_path, exist_ok=True)\n",
    "\n",
    "        result_lines = pd.concat(\n",
    "                [\n",
    "                    pathline_to_line_geopandas_dataframe(pathlines[k][0], attrs={\"pathline_id\": [k]})\n",
    "                    for k, _ in starting_points_df.iterrows()\n",
    "                ]\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "        # Filter out broken geometries\n",
    "        result_lines = result_lines[\n",
    "            (result_lines.geometry.bounds.minx > -700_000) & (result_lines.geometry.bounds.maxx < 700_000) &\n",
    "            (result_lines.geometry.bounds.miny > -3_000_000) & (result_lines.geometry.bounds.maxy < -1_000_000)\n",
    "        ]\n",
    "\n",
    "        # Save GeoJSON\n",
    "        filename = f\"fl_{feature_id}_lines_v3.geojson\"\n",
    "        filepath = os.path.join(fl_lines_path, filename)\n",
    "        result_lines.to_file(filepath, driver='GeoJSON')\n",
    "\n",
    "        print(f\"For Glacier {glacier_name} (ID:{feature_id}) generated {len(result_lines)} flowlines (with {len(result)} points).\")\n",
    " \n",
    "\n",
    "        # === GET FLOWLINES STATISTICS ===\n",
    "\n",
    "        # Aggregate metrics\n",
    "        summary = result.groupby(\"pathline_id\").agg(\n",
    "            v_sum=(\"v\", \"sum\"),\n",
    "            v_avg=(\"v\", \"mean\"),\n",
    "            num_points=(\"v\", \"count\"),\n",
    "            stopped_at_time=(\"time\", \"max\"),\n",
    "            mean_error=(\"error\", \"mean\"),\n",
    "            mean_distance=(\"distance\", \"mean\")\n",
    "        ).reset_index()\n",
    "\n",
    "        # For stopped_at_distance, get last point per pathline\n",
    "        last_points = result.sort_values(\"time\").groupby(\"pathline_id\").last().reset_index()\n",
    "        last_points = last_points[[\"pathline_id\", \"distance_from_origin\"]].rename(\n",
    "            columns={\"distance_from_origin\": \"stopped_at_distance\"}\n",
    "        )\n",
    "\n",
    "        # Merge summary and last point info\n",
    "        summary = pd.merge(summary, last_points, on=\"pathline_id\")\n",
    "\n",
    "        # Get metadata from the first point\n",
    "        first_points = result.sort_values(\"time\").groupby(\"pathline_id\").first().reset_index()\n",
    "        first_points = first_points.rename(columns={\n",
    "            \"x\": \"x_start\",\n",
    "            \"y\": \"y_start\"\n",
    "        })\n",
    "\n",
    "        meta_cols = [\"pathline_id\", \"glacier_ID\", \"feature_ID\", \"glacier_name\", \"x_start\", \"y_start\"]\n",
    "        first_points = first_points[meta_cols]\n",
    "\n",
    "        # Merge everything\n",
    "        flowline_stats = pd.merge(first_points, summary, on=\"pathline_id\")\n",
    "\n",
    "        # Reorder columns as requested\n",
    "        desired_order = [\n",
    "            \"pathline_id\",\n",
    "            \"glacier_ID\",\n",
    "            \"feature_ID\",\n",
    "            \"glacier_name\",\n",
    "            \"x_start\",\n",
    "            \"y_start\",\n",
    "            \"num_points\",\n",
    "            \"v_sum\",\n",
    "            \"v_avg\",\n",
    "            \"mean_error\",\n",
    "            \"mean_distance\",\n",
    "            \"stopped_at_distance\",\n",
    "            \"stopped_at_time\"\n",
    "        ]\n",
    "        flowline_stats = flowline_stats[desired_order]\n",
    "\n",
    "        # Output path\n",
    "        stats_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Statistics/Flowlines'\n",
    "        os.makedirs(stats_path, exist_ok=True)\n",
    "        filename = f\"stats_fl_{first_points['feature_ID'].iloc[0]}_v3.csv\"\n",
    "        filepath = os.path.join(stats_path, filename)\n",
    "\n",
    "        # Save CSV\n",
    "        flowline_stats.to_csv(filepath, index=False)\n",
    "        print(f\"For Glacier {glacier_name} (ID:{feature_id}) saved flowline statistics.\")\n",
    "\n",
    "\n",
    "        # === SAVE FASTEST FLOWLINES AS LINES ===\n",
    "\n",
    "        # Identify pathline_ids with highest v_sum and v_mean\n",
    "        max_sum_id = flowline_stats.sort_values(\"v_sum\", ascending=False).iloc[0][\"pathline_id\"]\n",
    "        max_avg_id = flowline_stats.sort_values(\"v_avg\", ascending=False).iloc[0][\"pathline_id\"]\n",
    "\n",
    "        # Create output directories if needed\n",
    "        out_dir_sum = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Flowlines/Fastest Sum'\n",
    "        os.makedirs(out_dir_sum, exist_ok=True)\n",
    "        out_dir_avg = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Flowlines/Fastest Avg'\n",
    "        os.makedirs(out_dir_avg, exist_ok=True)\n",
    "\n",
    "        # Extract the corresponding line geometries\n",
    "        line_max_sum = pathline_to_line_geopandas_dataframe(\n",
    "            pathlines[max_sum_id][0], attrs={\"pathline_id\": [max_sum_id]}\n",
    "        )\n",
    "        line_max_avg = pathline_to_line_geopandas_dataframe(\n",
    "            pathlines[max_avg_id][0], attrs={\"pathline_id\": [max_avg_id]}\n",
    "        )\n",
    "\n",
    "        # Save as GeoJSON\n",
    "        filename_sum = f\"fl_{feature_id}_lines_max_sum_v3.geojson\"\n",
    "        filepath_sum = os.path.join(out_dir_sum, filename_sum)\n",
    "        line_max_sum.to_file(filepath_sum, driver='GeoJSON')\n",
    "\n",
    "        filename_avg = f\"fl_{feature_id}_lines_max_avg_v3.geojson\"\n",
    "        filepath_avg = os.path.join(out_dir_avg, filename_avg)\n",
    "        line_max_avg.to_file(filepath_avg, driver='GeoJSON')\n",
    "\n",
    "\n",
    "        # === SAVE PLOT WITH DOTS ===\n",
    "\n",
    "        # Create output folder if not exists\n",
    "        plot_points_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Graphs/with Points'\n",
    "        os.makedirs(plot_points_path, exist_ok=True)\n",
    "\n",
    "        # Compute square bounds\n",
    "        minx, miny, maxx, maxy = result.total_bounds\n",
    "        center_x = (minx + maxx) / 2\n",
    "        center_y = (miny + maxy) / 2\n",
    "\n",
    "        width = maxx - minx\n",
    "        height = maxy - miny\n",
    "        max_extent = max(width, height) / 2 + 5000  # buffer of 5 km on each side\n",
    "\n",
    "        minx = center_x - max_extent\n",
    "        maxx = center_x + max_extent\n",
    "        miny = center_y - max_extent\n",
    "        maxy = center_y + max_extent\n",
    "\n",
    "        # Flip y and Vv if needed (raster is top-down)\n",
    "        if y[0] > y[-1]:\n",
    "            y = y[::-1]\n",
    "            Vv = Vv[::-1, :]\n",
    "\n",
    "        # Wrap into xarray DataArray\n",
    "        Vv = xr.DataArray(Vv, coords=[(\"y\", y), (\"x\", x)])\n",
    "        Vv_crop = Vv.sel(x=slice(minx, maxx), y=slice(miny, maxy))\n",
    "\n",
    "        # Compute ratio for correct aspect\n",
    "        width = maxx - minx\n",
    "        height = maxy - miny\n",
    "        # Check for valid numbers\n",
    "        if width > 0 and height > 0:\n",
    "            ratio = height / width\n",
    "        else:\n",
    "            raise ValueError(\"Invalid bounds: cannot compute aspect ratio.\")\n",
    "\n",
    "        NWCW = gp.read_file('data/kuba-flowlines/NW-CW_Basins.shp')\n",
    "        contour100m = gp.read_file('data/kuba-flowlines/contour_100masl_arcticdem_10m_v4.1_nwcw.shp')\n",
    "        contour1500m = gp.read_file('data/kuba-flowlines/contour_1500masl_arcticdem_10m_v4.1_nwcw.shp')\n",
    "        contour = gp.read_file('data/kuba-flowlines/contour_every100_arcticdem_10m_v4.1_nwcw.shp')\n",
    "\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "        # Background raster\n",
    "        Vv_crop.plot(\n",
    "            ax=ax,\n",
    "            cmap=\"speed_colorblind\",\n",
    "            vmin=10,\n",
    "            vmax=1500,\n",
    "            zorder=1,\n",
    "            cbar_kwargs={\n",
    "                \"shrink\": 0.6,\n",
    "                \"label\": r\"velocity [m a$^{-1}$]\",\n",
    "                \"pad\": 0.02\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Contours\n",
    "        contour.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=0.25, zorder=2)\n",
    "        contour100m.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=1, zorder=2)\n",
    "        contour1500m.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=1, zorder=2)\n",
    "\n",
    "        # NWCW outline (on top of contours)\n",
    "        NWCW.plot(ax=ax, facecolor=\"none\", edgecolor=\"g\", linewidth=1, zorder=3)\n",
    "\n",
    "        # Flowlines\n",
    "        result.plot(ax=ax, color=\"w\", edgecolor=\"k\", linewidth=0.00, markersize = 0.5, zorder=4)\n",
    "\n",
    "        # Starting points (on top)\n",
    "        starting_points_df.plot(ax=ax, color=\"black\", edgecolor=\"k\", linewidth=0.01, markersize=0.5, zorder=5)\n",
    "\n",
    "        # Labels\n",
    "        ax.set_xlabel(\"x coordinate of projection [m]\", fontsize=12)\n",
    "        ax.set_ylabel(\"y coordinate of projection [m]\", fontsize=12)\n",
    "        ax.set_title(f\"ID: {feature_id}, {glacier_name}\", fontsize=16)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        output_path = os.path.join(plot_points_path, f\"fl_{feature_id}_points_v3.pdf\")\n",
    "        plt.savefig(output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "        # === SAVE PLOT WITH LINES ===\n",
    "\n",
    "        # Create output folder if not exists\n",
    "        plot_lines_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Graphs/with Lines'\n",
    "        os.makedirs(plot_lines_path, exist_ok=True)\n",
    "\n",
    "        # Compute square bounds\n",
    "        minx, miny, maxx, maxy = result.total_bounds\n",
    "        center_x = (minx + maxx) / 2\n",
    "        center_y = (miny + maxy) / 2\n",
    "\n",
    "        width = maxx - minx\n",
    "        height = maxy - miny\n",
    "        max_extent = max(width, height) / 2 + 5000  # buffer of 5 km on each side\n",
    "\n",
    "        minx = center_x - max_extent\n",
    "        maxx = center_x + max_extent\n",
    "        miny = center_y - max_extent\n",
    "        maxy = center_y + max_extent\n",
    "\n",
    "        # Flip y and Vv if needed (raster is top-down)\n",
    "        if y[0] > y[-1]:\n",
    "            y = y[::-1]\n",
    "            Vv = Vv[::-1, :]\n",
    "\n",
    "        # Wrap into xarray DataArray\n",
    "        Vv = xr.DataArray(Vv, coords=[(\"y\", y), (\"x\", x)])\n",
    "\n",
    "        Vv_crop = Vv.sel(x=slice(minx, maxx), y=slice(miny, maxy))\n",
    "\n",
    "        # Compute ratio for correct aspect\n",
    "        width = maxx - minx\n",
    "        height = maxy - miny\n",
    "        # Check for valid numbers\n",
    "        if width > 0 and height > 0:\n",
    "            ratio = height / width\n",
    "        else:\n",
    "            raise ValueError(\"Invalid bounds: cannot compute aspect ratio.\")\n",
    "\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "        # Background raster\n",
    "        Vv_crop.plot(\n",
    "            ax=ax,\n",
    "            cmap=\"speed_colorblind\",\n",
    "            vmin=10,\n",
    "            vmax=1500,\n",
    "            zorder=1,\n",
    "            cbar_kwargs={\n",
    "                \"shrink\": 0.6,\n",
    "                \"label\": r\"velocity [m a$^{-1}$]\",\n",
    "                \"pad\": 0.02\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Contours\n",
    "        contour.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=0.25, zorder=2)\n",
    "        contour100m.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=1, zorder=2)\n",
    "        contour1500m.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=1, zorder=2)\n",
    "\n",
    "        # NWCW outline (on top of contours)\n",
    "        NWCW.plot(ax=ax, facecolor=\"none\", edgecolor=\"g\", linewidth=1, zorder=3)\n",
    "\n",
    "        # Flowlines\n",
    "        result_lines.plot(ax=ax, color=\"w\", edgecolor=\"k\", linewidth=0.75, zorder=4)\n",
    "\n",
    "        # Starting points (on top)\n",
    "        starting_points_df.plot(ax=ax, color=\"black\", edgecolor=\"k\", linewidth=0.01, markersize=0.5, zorder=5)\n",
    "\n",
    "        # Labels\n",
    "        ax.set_xlabel(\"x coordinate of projection [m]\", fontsize=12)\n",
    "        ax.set_ylabel(\"y coordinate of projection [m]\", fontsize=12)\n",
    "        ax.set_title(f\"ID: {feature_id}, {glacier_name}\", fontsize=16)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        output_path = os.path.join(plot_lines_path, f\"fl_{feature_id}_lines_v3.pdf\")\n",
    "        plt.savefig(output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        # === SAVE PLOT WITH LINES (WITH CHAD'S IDs) ===\n",
    "\n",
    "        # Create output folder if not exists\n",
    "        plot_lines_chad_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Graphs/with Lines Chad'\n",
    "        os.makedirs(plot_lines_chad_path, exist_ok=True)\n",
    "\n",
    "        # Compute square bounds\n",
    "        minx, miny, maxx, maxy = result.total_bounds\n",
    "        center_x = (minx + maxx) / 2\n",
    "        center_y = (miny + maxy) / 2\n",
    "\n",
    "        width = maxx - minx\n",
    "        height = maxy - miny\n",
    "        max_extent = max(width, height) / 2 + 5000  # buffer of 5 km on each side\n",
    "\n",
    "        minx = center_x - max_extent\n",
    "        maxx = center_x + max_extent\n",
    "        miny = center_y - max_extent\n",
    "        maxy = center_y + max_extent\n",
    "\n",
    "        # Flip y and Vv if needed (raster is top-down)\n",
    "        if y[0] > y[-1]:\n",
    "            y = y[::-1]\n",
    "            Vv = Vv[::-1, :]\n",
    "\n",
    "        # Wrap into xarray DataArray\n",
    "        Vv = xr.DataArray(Vv, coords=[(\"y\", y), (\"x\", x)])\n",
    "\n",
    "        Vv_crop = Vv.sel(x=slice(minx, maxx), y=slice(miny, maxy))\n",
    "\n",
    "        # Compute ratio for correct aspect\n",
    "        width = maxx - minx\n",
    "        height = maxy - miny\n",
    "        # Check for valid numbers\n",
    "        if width > 0 and height > 0:\n",
    "            ratio = height / width\n",
    "        else:\n",
    "            raise ValueError(\"Invalid bounds: cannot compute aspect ratio.\")\n",
    "\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "        # Background raster\n",
    "        Vv_crop.plot(\n",
    "            ax=ax,\n",
    "            cmap=\"speed_colorblind\",\n",
    "            vmin=10,\n",
    "            vmax=1500,\n",
    "            zorder=1,\n",
    "            cbar_kwargs={\n",
    "                \"shrink\": 0.6,\n",
    "                \"label\": r\"velocity [m a$^{-1}$]\",\n",
    "                \"pad\": 0.02\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Contours\n",
    "        contour.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=0.25, zorder=2)\n",
    "        contour100m.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=1, zorder=2)\n",
    "        contour1500m.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=1, zorder=2)\n",
    "\n",
    "        # NWCW outline (on top of contours)\n",
    "        NWCW.plot(ax=ax, facecolor=\"none\", edgecolor=\"g\", linewidth=1, zorder=3)\n",
    "\n",
    "        # Flowlines\n",
    "        result_lines.plot(ax=ax, color=\"w\", edgecolor=\"k\", linewidth=0.75, zorder=4)\n",
    "\n",
    "        # Starting points (on top)\n",
    "        starting_points_df.plot(ax=ax, color=\"black\", edgecolor=\"k\", linewidth=0.01, markersize=0.5, zorder=5)\n",
    "\n",
    "        # Labels\n",
    "        ax.set_xlabel(\"x coordinate of projection [m]\", fontsize=12)\n",
    "        ax.set_ylabel(\"y coordinate of projection [m]\", fontsize=12)\n",
    "        ax.set_title(f\"ID: {feature_id + 1}, {glacier_name}\", fontsize=16)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        output_path = os.path.join(plot_lines_chad_path, f\"fl_{feature_id}_lines_chad_v3.pdf\")\n",
    "        plt.savefig(output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"For Glacier {glacier_name} (ID:{feature_id}) saved plots).\") \n",
    "\n",
    "        # === COMPUTE AND SAVE GLACIER STATISTICS ===\n",
    "\n",
    "        # Compute per-glacier statistics\n",
    "        stats_row = {\n",
    "            \"glacier_ID\": glacier_id,\n",
    "            \"feature_ID\": feature_id,\n",
    "            \"glacier_name\": glacier_name,\n",
    "            \"num_sp\": len(starting_points_df),\n",
    "            \"num_fl\": len(result_lines),\n",
    "            \"num_fl_points\": len(result),\n",
    "            \"failed_flowlines\": len(starting_points_df) - len(result_lines)\n",
    "        }\n",
    "\n",
    "        if not flowline_stats.empty:\n",
    "            stats_row[\"fastest_fl_sum\"] = flowline_stats.sort_values(\"v_sum\", ascending=False).iloc[0][\"pathline_id\"]\n",
    "            stats_row[\"fastest_fl_avg\"] = flowline_stats.sort_values(\"v_avg\", ascending=False).iloc[0][\"pathline_id\"]\n",
    "            stats_row[\"longest_fl\"] = flowline_stats.sort_values(\"stopped_at_distance\", ascending=False).iloc[0][\"pathline_id\"]\n",
    "        else:\n",
    "            stats_row[\"fastest_fl_sum\"] = None\n",
    "            stats_row[\"fastest_fl_avg\"] = None\n",
    "            stats_row[\"longest_fl\"] = None\n",
    "\n",
    "        # Append to global stats list\n",
    "        if \"all_glacier_stats\" not in globals():\n",
    "            all_glacier_stats = []\n",
    "\n",
    "        all_glacier_stats.append(stats_row) \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(print(f\"Failed processing Glacier {glacier_name} (ID: {feature_id}): {e}\"))\n",
    "\n",
    "stats_df = pd.DataFrame(all_glacier_stats)\n",
    "stats_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Statistics/Glaciers'\n",
    "os.makedirs(stats_path, exist_ok=True)\n",
    "filepath = os.path.join(stats_path, \"stats_gl_1_v3.csv\")\n",
    "stats_df.to_csv(filepath, index=False)\n",
    "\n",
    "print(f\"Saved all-glacier statistics.\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picked Flowlines Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Input paths\n",
    "picks_csv_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Flowlines of Interest/fl_picks_3.1.csv'\n",
    "flowlines_dir = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Flowlines/as Lines'\n",
    "output_dir = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Flowlines of Interest/Picks'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load picks CSV\n",
    "picks_df = pd.read_csv(picks_csv_path)\n",
    "\n",
    "# Iterate over each row (each glacier/feature_ID)\n",
    "for _, row in picks_df.iterrows():\n",
    "    feature_id = row['feature_ID']\n",
    "    flowline_file = os.path.join(flowlines_dir, f\"fl_{feature_id}_lines_v3.geojson\")\n",
    "\n",
    "    if not os.path.exists(flowline_file):\n",
    "        print(f\"⚠️ Flowline file missing: {flowline_file}\")\n",
    "        continue\n",
    "\n",
    "    # Load flowlines for current feature_ID\n",
    "    gdf = gpd.read_file(flowline_file)\n",
    "\n",
    "    # Check all pathline_id columns (pathline_id_1, pathline_id_2, ...)\n",
    "    for col in row.index:\n",
    "        if col.startswith(\"pathline_id_\") and pd.notna(row[col]):\n",
    "            pathline_id = int(row[col])\n",
    "            flowline_number = col.split(\"_\")[-1]  # e.g., '1', '2', etc.\n",
    "\n",
    "            # Filter flowline with specific pathline_id\n",
    "            subset = gdf[gdf['pathline_id'] == pathline_id]\n",
    "            if subset.empty:\n",
    "                print(f\"Pathline_id {pathline_id} not found in fl_{feature_id}_lines_v3.geojson\")\n",
    "                continue\n",
    "\n",
    "            # Save each selected flowline as separate GeoJSON\n",
    "            out_name = f\"gl_{feature_id}_{flowline_number}.geojson\"\n",
    "            out_path = os.path.join(output_dir, out_name)\n",
    "            subset.to_file(out_path, driver='GeoJSON')\n",
    "            print(f\"Saved: {out_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flowline Graphs with Picked Flowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "all_fl_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Flowlines/as Lines/Store'\n",
    "picked_fl_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Flowlines of Interest/Picks'\n",
    "sp_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Input/Starting Points'\n",
    "out_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Flowlines of Interest/Graphs with Picks/with Lines'\n",
    "out_chad_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Flowlines of Interest/Graphs with Picks/with Lines Chad'\n",
    "titles_csv = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Flowlines of Interest/fl_picks_3.1.csv'\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "os.makedirs(out_chad_path, exist_ok=True)\n",
    "\n",
    "# === Static overlays ===\n",
    "NWCW = gp.read_file('data/kuba-flowlines/NW-CW_Basins.shp')\n",
    "contour100m = gp.read_file('data/kuba-flowlines/contour_100masl_arcticdem_10m_v4.1_nwcw.shp')\n",
    "contour1500m = gp.read_file('data/kuba-flowlines/contour_1500masl_arcticdem_10m_v4.1_nwcw.shp')\n",
    "contour = gp.read_file('data/kuba-flowlines/contour_every100_arcticdem_10m_v4.1_nwcw.shp')\n",
    "\n",
    "# === Raster ===\n",
    "with rasterio.open(\"data/kuba-flowlines/GL_vel_mosaic_Annual_01Dec15_30Nov21_vv_v05.0_nwcw_average.tif\") as src_vv:\n",
    "    Vv = src_vv.read(1)\n",
    "    transform = src_vv.transform\n",
    "    x = (np.arange(src_vv.width) * transform.a) + transform.c\n",
    "    y = (np.arange(src_vv.height) * transform.e) + transform.f\n",
    "\n",
    "if y[0] > y[-1]:\n",
    "    y = y[::-1]\n",
    "    Vv = Vv[::-1, :]\n",
    "\n",
    "Vv = xr.DataArray(Vv, coords=[(\"y\", y), (\"x\", x)])\n",
    "\n",
    "# === Metadata ===\n",
    "titles_df = pd.read_csv(titles_csv)\n",
    "\n",
    "# === Loop ===\n",
    "for _, row in titles_df.iterrows():\n",
    "    feature_id = int(row[\"feature_ID\"])\n",
    "    glacier_name = row[\"glacier_name\"]\n",
    "\n",
    "    try:\n",
    "        fl_file = f\"fl_{feature_id}_lines_v3.geojson\"\n",
    "        all_fl = gp.read_file(os.path.join(all_fl_path, fl_file))\n",
    "\n",
    "        sp_file = f\"sp_{feature_id}.geojson\"\n",
    "        starting_points_df = gp.read_file(os.path.join(sp_path, sp_file))\n",
    "\n",
    "        picks = []\n",
    "        for fname in os.listdir(picked_fl_path):\n",
    "            if fname.startswith(f\"gl_{feature_id}_\") and fname.endswith(\".geojson\"):\n",
    "                picks.append(gp.read_file(os.path.join(picked_fl_path, fname)))\n",
    "\n",
    "        if not picks:\n",
    "            print(f\"No picks found for glacier {feature_id}. Skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        picks_gdf = pd.concat(picks).reset_index(drop=True)\n",
    "\n",
    "        # Bounds\n",
    "        minx, miny, maxx, maxy = all_fl.total_bounds\n",
    "        center_x = (minx + maxx) / 2\n",
    "        center_y = (miny + maxy) / 2\n",
    "        width = maxx - minx\n",
    "        height = maxy - miny\n",
    "        max_extent = max(width, height) / 2 + 5000\n",
    "        minx = center_x - max_extent\n",
    "        maxx = center_x + max_extent\n",
    "        miny = center_y - max_extent\n",
    "        maxy = center_y + max_extent\n",
    "\n",
    "        Vv_crop = Vv.sel(x=slice(minx, maxx), y=slice(miny, maxy))\n",
    "\n",
    "        for suffix, title_id, out_dir in [(\"\", feature_id, out_path), (\"_chad\", feature_id + 1, out_chad_path)]:\n",
    "            fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "            ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "            Vv_crop.plot(\n",
    "                ax=ax,\n",
    "                cmap=\"speed_colorblind\",\n",
    "                vmin=10,\n",
    "                vmax=1500,\n",
    "                zorder=1,\n",
    "                cbar_kwargs={\"shrink\": 0.6, \"label\": r\"velocity [m a$^{-1}$]\", \"pad\": 0.02}\n",
    "            )\n",
    "\n",
    "            contour.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=0.25, zorder=2)\n",
    "            contour100m.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=1, zorder=2)\n",
    "            contour1500m.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=1, zorder=2)\n",
    "            NWCW.plot(ax=ax, facecolor=\"none\", edgecolor=\"g\", linewidth=1, zorder=3)\n",
    "            all_fl.plot(ax=ax, color=\"w\", edgecolor=\"k\", linewidth=0.75, zorder=4)\n",
    "            picks_gdf.plot(ax=ax, color=\"black\", edgecolor=\"k\", linewidth=2.5, zorder=5)\n",
    "            starting_points_df.plot(ax=ax, color=\"black\", edgecolor=\"k\", linewidth=0.01, markersize=0.5, zorder=6)\n",
    "\n",
    "            ax.set_xlabel(\"x coordinate of projection [m]\", fontsize=12)\n",
    "            ax.set_ylabel(\"y coordinate of projection [m]\", fontsize=12)\n",
    "            ax.set_title(f\"ID: {title_id}, {glacier_name}\", fontsize=16)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            filename = f\"fl_{feature_id}_picked_lines{suffix}_v3.pdf\"\n",
    "            output_path = os.path.join(out_dir, filename)\n",
    "            plt.savefig(output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        print(f\"Saved plots for Glacier {glacier_name} ({feature_id}).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Glacier {glacier_name} ({feature_id}): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flowline Graphs with Picked Flowlines and Elevation Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "\n",
    "# === Paths ===\n",
    "all_fl_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Output/Flowlines/as Lines'\n",
    "picked_fl_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Flowlines of Interest/Picks/Store'\n",
    "int_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Points/Intersections Cleaned'\n",
    "buff_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Points/Buffers'\n",
    "out_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Flowlines of Interest/Graphs with Picks and Buffers/with Lines'\n",
    "out_chad_path = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Flowlines of Interest/Graphs with Picks and Buffers/with Lines Chad'\n",
    "titles_csv = '/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Flowlines of Interest/fl_picks_3.1.csv'\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "os.makedirs(out_chad_path, exist_ok=True)\n",
    "\n",
    "# === Static overlays ===\n",
    "NWCW = gp.read_file('data/kuba-flowlines/NW-CW_Basins.shp')\n",
    "contour100m = gp.read_file('data/kuba-flowlines/contour_100masl_arcticdem_10m_v4.1_nwcw.shp')\n",
    "contour1500m = gp.read_file('data/kuba-flowlines/contour_1500masl_arcticdem_10m_v4.1_nwcw.shp')\n",
    "contour = gp.read_file('data/kuba-flowlines/contour_every100_arcticdem_10m_v4.1_nwcw.shp')\n",
    "\n",
    "# === Raster ===\n",
    "with rasterio.open(\"data/kuba-flowlines/GL_vel_mosaic_Annual_01Dec15_30Nov21_vv_v05.0_nwcw_average.tif\") as src_vv:\n",
    "    Vv = src_vv.read(1)\n",
    "    transform = src_vv.transform\n",
    "    x = (np.arange(src_vv.width) * transform.a) + transform.c\n",
    "    y = (np.arange(src_vv.height) * transform.e) + transform.f\n",
    "\n",
    "if y[0] > y[-1]:\n",
    "    y = y[::-1]\n",
    "    Vv = Vv[::-1, :]\n",
    "\n",
    "Vv = xr.DataArray(Vv, coords=[(\"y\", y), (\"x\", x)])\n",
    "\n",
    "# === Metadata ===\n",
    "titles_df = pd.read_csv(titles_csv)\n",
    "\n",
    "# === Loop ===\n",
    "for _, row in titles_df.iterrows():\n",
    "    feature_id = int(row[\"feature_ID\"])\n",
    "    glacier_name = row[\"glacier_name\"]\n",
    "\n",
    "    try:\n",
    "        picks = []\n",
    "        intersections = []\n",
    "        buffers = []\n",
    "\n",
    "        for fname in os.listdir(picked_fl_path):\n",
    "            if fname.startswith(f\"gl_{feature_id}_\") and fname.endswith(\".geojson\"):\n",
    "                fl_id = fname.split(\"_\")[-1].split(\".\")[0]  # Extract flowline ID\n",
    "                picks.append(gp.read_file(os.path.join(picked_fl_path, fname)))\n",
    "\n",
    "                int_path_file = f\"intsect_clean_gl_{feature_id}_{fl_id}.geojson\"\n",
    "                buff_path_file = f\"buff_intsect_clean_gl_{feature_id}_{fl_id}.geojson\"\n",
    "\n",
    "                if os.path.exists(os.path.join(int_path, int_path_file)):\n",
    "                    intersections.append(gp.read_file(os.path.join(int_path, int_path_file)))\n",
    "                if os.path.exists(os.path.join(buff_path, buff_path_file)):\n",
    "                    buffers.append(gp.read_file(os.path.join(buff_path, buff_path_file)))\n",
    "\n",
    "        if not picks:\n",
    "            print(f\"No picks found for glacier {feature_id}. Skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        picks_gdf = pd.concat(picks).reset_index(drop=True)\n",
    "        intersections_gdf = pd.concat(intersections).reset_index(drop=True) if intersections else None\n",
    "        buffers_gdf = pd.concat(buffers).reset_index(drop=True) if buffers else None\n",
    "\n",
    "        minx, miny, maxx, maxy = picks_gdf.total_bounds\n",
    "        center_x = (minx + maxx) / 2\n",
    "        center_y = (miny + maxy) / 2\n",
    "        width = maxx - minx\n",
    "        height = maxy - miny\n",
    "        max_extent = max(width, height) / 2 + 5000\n",
    "        minx = center_x - max_extent\n",
    "        maxx = center_x + max_extent\n",
    "        miny = center_y - max_extent\n",
    "        maxy = center_y + max_extent\n",
    "\n",
    "        Vv_crop = Vv.sel(x=slice(minx, maxx), y=slice(miny, maxy))\n",
    "\n",
    "        for suffix, title_id, out_dir in [(\"\", feature_id, out_path), (\"_chad\", feature_id + 1, out_chad_path)]:\n",
    "            fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "            ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "            Vv_crop.plot(\n",
    "                ax=ax,\n",
    "                cmap=\"speed_colorblind\",\n",
    "                vmin=10,\n",
    "                vmax=1500,\n",
    "                zorder=1,\n",
    "                cbar_kwargs={\"shrink\": 0.6, \"label\": r\"velocity [m a$^{-1}$]\", \"pad\": 0.02}\n",
    "            )\n",
    "\n",
    "            contour.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=0.25, zorder=3)\n",
    "            contour100m.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=1, zorder=3)\n",
    "            contour1500m.plot(ax=ax, color=\"b\", edgecolor=\"k\", linewidth=1, zorder=3)\n",
    "            NWCW.plot(ax=ax, facecolor=\"none\", edgecolor=\"g\", linewidth=1, zorder=4)\n",
    "            picks_gdf.plot(ax=ax, color=\"black\", edgecolor=\"k\", linewidth=2.5, zorder=5)\n",
    "\n",
    "            if buffers_gdf is not None:\n",
    "                buffers_gdf.plot(ax=ax, color=\"green\", edgecolor=\"k\", linewidth=0.5, alpha=0.5, zorder=2)\n",
    "\n",
    "            if intersections_gdf is not None:\n",
    "                intersections_gdf.plot(ax=ax, color=\"white\", edgecolor=\"k\", linewidth=0.01, markersize=5, zorder=6)\n",
    "\n",
    "            ax.set_xlabel(\"x coordinate of projection [m]\", fontsize=12)\n",
    "            ax.set_ylabel(\"y coordinate of projection [m]\", fontsize=12)\n",
    "            ax.set_title(f\"ID: {title_id}, {glacier_name}\", fontsize=16)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            filename = f\"fl_{feature_id}_buffers{suffix}_v3.pdf\"\n",
    "            output_path = os.path.join(out_dir, filename)\n",
    "            plt.savefig(output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "\n",
    "        print(f\"Saved plots for Glacier {glacier_name} ({feature_id}).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Glacier {glacier_name} ({feature_id}): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Sampling Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "input_dir = Path('/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Points/Random Points')\n",
    "geojson_out_dir = Path('/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Points/Sampling Points/as GeoJSON')\n",
    "csv_out_dir = Path('/Users/jagon/Documents/Projects/Collabs/Jessica Badgeley/New Points v3/Points/Sampling Points/as CSV')\n",
    "\n",
    "# Process each GeoJSON file\n",
    "for file in input_dir.glob(\"rp_buff_intsect_clean_gl_*.geojson\"):\n",
    "    try:\n",
    "        gdf = gp.read_file(file)\n",
    "\n",
    "        # Extract glacier and flowline ID from filename\n",
    "        parts = file.stem.split(\"gl_\")[-1].split(\"_\")\n",
    "        glacier_id = parts[0]\n",
    "        flowline_id = parts[1]\n",
    "        base_name = f\"gl_{glacier_id}_{flowline_id}\"\n",
    "\n",
    "        # Create subdirectories for glacier ID and flowline ID\n",
    "        glacier_geojson_dir = geojson_out_dir / f\"Glacier {glacier_id} Flowline {flowline_id}\"\n",
    "        glacier_csv_dir = csv_out_dir / f\"Glacier {glacier_id} Flowline {flowline_id}\"\n",
    "        glacier_geojson_dir.mkdir(parents=True, exist_ok=True)\n",
    "        glacier_csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Group by elevation\n",
    "        grouped = gdf.groupby(gdf[\"ELEV\"].astype(int))\n",
    "\n",
    "        for elev, group in grouped:\n",
    "            group = group.copy()\n",
    "            elev_str = str(int(elev))\n",
    "            geojson_path = glacier_geojson_dir / f\"{base_name}_{elev_str}.geojson\"\n",
    "            csv_path = glacier_csv_dir / f\"{base_name}_{elev_str}.csv\"\n",
    "\n",
    "            # Save grouped GeoDataFrame to GeoJSON\n",
    "            group.to_file(geojson_path, driver=\"GeoJSON\")\n",
    "\n",
    "            # Add x, y in EPSG:3413\n",
    "            group[\"x_3413\"] = group.geometry.x\n",
    "            group[\"y_3413\"] = group.geometry.y\n",
    "\n",
    "            # Reproject and extract x, y in EPSG:4326\n",
    "            group_wgs84 = group.to_crs(epsg=4326)\n",
    "            group[\"x_4326\"] = group_wgs84.geometry.x\n",
    "            group[\"y_4326\"] = group_wgs84.geometry.y\n",
    "\n",
    "            # Save to CSV with coordinates\n",
    "            group.drop(columns=\"geometry\").to_csv(csv_path, index=False)\n",
    "\n",
    "        print(f\"Processed {file.name} into {len(grouped)} elevation groups.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file.name}: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacier-flow-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
